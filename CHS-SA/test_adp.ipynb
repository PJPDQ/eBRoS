{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f902335-cf65-4c42-9e59-59786468d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873892a6-656b-4908-ab2a-036f5a600685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class Environment:\n",
    "    \"\"\"Environment for EV scheduling with stochastic transit delays and demand\"\"\"\n",
    "    \n",
    "    def __init__(self, num_buses=10, num_routes=5, max_charge=100, \n",
    "                 charge_rate=10, discharge_rate_per_km=0.5, max_delay=30):\n",
    "        # Environment parameters\n",
    "        self.num_buses = num_buses\n",
    "        self.num_routes = num_routes\n",
    "        self.max_charge = max_charge  # Maximum battery charge level (kWh)\n",
    "        self.charge_rate = charge_rate  # kWh per time step when charging\n",
    "        self.discharge_rate_per_km = discharge_rate_per_km  # kWh per km\n",
    "        self.max_delay = max_delay  # Maximum possible delay in minutes\n",
    "        \n",
    "        # Route properties\n",
    "        self.route_distances = np.random.uniform(5, 20, num_routes)  # km\n",
    "        self.route_times = self.route_distances * 3  # minutes (assuming 20 km/h)\n",
    "        \n",
    "        # Bus depot locations (for simplicity, assume single depot)\n",
    "        self.depot_location = np.array([0, 0])\n",
    "        \n",
    "        # Time-dependent traffic patterns (24 hours)\n",
    "        self.traffic_patterns = self._generate_traffic_patterns()\n",
    "        \n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "    \n",
    "    def _generate_traffic_patterns(self):\n",
    "        \"\"\"Generate 24-hour traffic patterns with AM and PM peaks\"\"\"\n",
    "        hours = np.arange(24)\n",
    "        # Morning peak (8 AM) and evening peak (5 PM)\n",
    "        pattern = 1 + 0.5 * np.exp(-0.5 * ((hours - 8) / 2) ** 2) + 0.7 * np.exp(-0.5 * ((hours - 17) / 2) ** 2)\n",
    "        return pattern\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        # Bus properties: [location_x, location_y, charge_level, assigned_route, status]\n",
    "        # Status: 0=idle at depot, 1=in service, 2=charging\n",
    "        self.buses = np.zeros((self.num_buses, 5))\n",
    "        self.buses[:, 2] = np.random.uniform(0.5, 1.0, self.num_buses) * self.max_charge  # Random initial charge\n",
    "        \n",
    "        # Time tracking\n",
    "        self.current_hour = 8  # Start at 8 AM\n",
    "        self.time_step = 0\n",
    "        \n",
    "        # Demand and delay tracking\n",
    "        self.current_demand = self._generate_demand()\n",
    "        self.current_delays = self._generate_delays()\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _generate_demand(self):\n",
    "        \"\"\"Generate stochastic demand for each route based on time of day\"\"\"\n",
    "        base_demand = np.random.poisson(10, self.num_routes)\n",
    "        time_factor = 1 + 0.5 * np.sin(np.pi * self.current_hour / 12)\n",
    "        return base_demand * time_factor\n",
    "    \n",
    "    def _generate_delays(self):\n",
    "        \"\"\"Generate stochastic delays for each route based on traffic patterns\"\"\"\n",
    "        # Use log-normal distribution for delay modeling\n",
    "        traffic_factor = self.traffic_patterns[int(self.current_hour)]\n",
    "        mean_delays = traffic_factor * np.random.uniform(1, 5, self.num_routes)\n",
    "        \n",
    "        # Log-normal distribution ensures delays are positive and can have occasional large values\n",
    "        delays = np.random.lognormal(mean=np.log(mean_delays), sigma=0.5)\n",
    "        return np.minimum(delays, self.max_delay)  # Cap at max_delay\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Return current state representation\"\"\"\n",
    "        # Flatten bus properties\n",
    "        bus_states = self.buses.flatten()\n",
    "        \n",
    "        # Current time (sin and cos encoding for cyclical nature)\n",
    "        time_sin = np.sin(2 * np.pi * self.current_hour / 24)\n",
    "        time_cos = np.cos(2 * np.pi * self.current_hour / 24)\n",
    "        \n",
    "        # Current demand and delays\n",
    "        demand = self.current_demand / 20.0  # Normalize\n",
    "        delays = self.current_delays / self.max_delay  # Normalize\n",
    "        \n",
    "        # Combine all state components\n",
    "        state = np.concatenate([\n",
    "            bus_states,\n",
    "            [time_sin, time_cos],\n",
    "            demand,\n",
    "            delays\n",
    "        ])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action and return new state, reward, done\n",
    "        Action is a matrix of assignments: [bus_id, route_id] or [bus_id, -1] for charging\n",
    "        \"\"\"\n",
    "        # Process each bus assignment\n",
    "        rewards = 0\n",
    "        \n",
    "        for bus_id, route_id in action:\n",
    "            bus_id = int(bus_id)\n",
    "            route_id = int(route_id)\n",
    "            \n",
    "            # Current bus state\n",
    "            bus = self.buses[bus_id]\n",
    "            charge_level = bus[2]\n",
    "            \n",
    "            if route_id == -1:  # Charging action\n",
    "                # Set bus to charging status\n",
    "                self.buses[bus_id, 3] = -1\n",
    "                self.buses[bus_id, 4] = 2\n",
    "                \n",
    "                # Increase charge level\n",
    "                new_charge = min(charge_level + self.charge_rate, self.max_charge)\n",
    "                self.buses[bus_id, 2] = new_charge\n",
    "                \n",
    "                # Small penalty for charging instead of serving\n",
    "                rewards -= 1\n",
    "                \n",
    "            elif 0 <= route_id < self.num_routes:  # Assign to route\n",
    "                route_distance = self.route_distances[route_id]\n",
    "                required_charge = route_distance * self.discharge_rate_per_km\n",
    "                \n",
    "                # Check if bus has enough charge\n",
    "                if charge_level >= required_charge:\n",
    "                    # Set bus to in-service status\n",
    "                    self.buses[bus_id, 3] = route_id\n",
    "                    self.buses[bus_id, 4] = 1\n",
    "                    \n",
    "                    # Decrease charge level\n",
    "                    self.buses[bus_id, 2] = charge_level - required_charge\n",
    "                    \n",
    "                    # Calculate service reward based on demand and delays\n",
    "                    demand_served = min(1.0, self.current_demand[route_id] / 10.0)\n",
    "                    delay_penalty = self.current_delays[route_id] / self.max_delay\n",
    "                    \n",
    "                    # Reward: balance between serving demand and avoiding delays\n",
    "                    route_reward = 10 * demand_served - 5 * delay_penalty\n",
    "                    rewards += route_reward\n",
    "                else:\n",
    "                    # Not enough charge - bus stays idle with penalty\n",
    "                    self.buses[bus_id, 4] = 0\n",
    "                    rewards -= 5\n",
    "            \n",
    "            else:  # Invalid route, bus stays idle\n",
    "                self.buses[bus_id, 4] = 0\n",
    "                rewards -= 1\n",
    "        \n",
    "        # Update time\n",
    "        self.time_step += 1\n",
    "        self.current_hour = (self.current_hour + 1) % 24\n",
    "        \n",
    "        # Update stochastic elements\n",
    "        self.current_demand = self._generate_demand()\n",
    "        self.current_delays = self._generate_delays()\n",
    "        \n",
    "        # Check if episode is done (e.g., after 24 hours)\n",
    "        done = self.time_step >= 24\n",
    "        \n",
    "        return self._get_state(), rewards, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ce649-29cc-4406-8387-a51778bbd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADPAgent:\n",
    "    \"\"\"Approximate Dynamic Programming agent using neural networks\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_buses, num_routes):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_buses = num_buses\n",
    "        self.num_routes = num_routes\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount factor\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        # Create value function approximator (Q-network)\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        # Target network for stability\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Neural network to approximate Q-value function\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Copy weights from model to target_model\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Choose action based on epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Explore: random assignment of buses to routes or charging\n",
    "            actions = []\n",
    "            for i in range(self.num_buses):\n",
    "                # Each bus can be assigned to a route (0 to num_routes-1) or to charging (-1)\n",
    "                route = random.randint(-1, self.num_routes - 1)\n",
    "                actions.append([i, route])\n",
    "            return np.array(actions)\n",
    "        \n",
    "        # Exploit: use model to predict best assignments\n",
    "        act_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        \n",
    "        # Convert 1D action values to bus-route assignments\n",
    "        actions = self._decode_action(act_values[0])\n",
    "        return actions\n",
    "    \n",
    "    def _decode_action(self, action_values):\n",
    "        \"\"\"Convert action values to bus-route assignments\"\"\"\n",
    "        # Reshape action values to a matrix [buses × (routes + charging)]\n",
    "        action_matrix = action_values.reshape(self.num_buses, self.num_routes + 1)\n",
    "        \n",
    "        assignments = []\n",
    "        for bus_id in range(self.num_buses):\n",
    "            # Get best action for this bus (either a route or charging=-1)\n",
    "            best_action = np.argmax(action_matrix[bus_id])\n",
    "            if best_action == self.num_routes:  # Last option is charging\n",
    "                route_id = -1\n",
    "            else:\n",
    "                route_id = best_action\n",
    "            \n",
    "            assignments.append([bus_id, route_id])\n",
    "        \n",
    "        return np.array(assignments)\n",
    "    \n",
    "    def _encode_action(self, assignments):\n",
    "        \"\"\"Convert bus-route assignments to flat action index\"\"\"\n",
    "        action_idx = 0\n",
    "        for bus_id, route_id in assignments:\n",
    "            # Convert route_id (-1 for charging) to 0...num_routes index\n",
    "            if route_id == -1:\n",
    "                route_idx = self.num_routes\n",
    "            else:\n",
    "                route_idx = route_id\n",
    "            \n",
    "            # Encode the assignment\n",
    "            position = bus_id * (self.num_routes + 1) + route_idx\n",
    "            action_idx |= (1 << position)\n",
    "        \n",
    "        return action_idx\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"Train model using experience replay\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # Calculate future Q-value using target network\n",
    "                next_q_values = self.target_model.predict(next_state.reshape(1, -1), verbose=0)[0]\n",
    "                target = reward + self.gamma * np.max(next_q_values)\n",
    "            \n",
    "            # Get current Q-values\n",
    "            current_q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "            \n",
    "            # Update the Q-value for the action taken\n",
    "            action_idx = self._encode_action(action)\n",
    "            current_q_values[0][action_idx] = target\n",
    "            \n",
    "            # Train the model\n",
    "            self.model.fit(state.reshape(1, -1), current_q_values, epochs=1, verbose=0)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a767f-182b-4a72-bade-93ec10910ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    \"\"\"Train the ADP agent\"\"\"\n",
    "    # Environment and agent setup\n",
    "    num_buses = 5\n",
    "    num_routes = 3\n",
    "    env = Environment(num_buses=num_buses, num_routes=num_routes)\n",
    "    \n",
    "    # Calculate state and action sizes\n",
    "    state_size = num_buses * 5 + 2 + num_routes * 2  # bus states + time encoding + demand + delays\n",
    "    action_size = 2 ** (num_buses * (num_routes + 1))  # Binary encoding of all possible assignments\n",
    "    \n",
    "    agent = ADPAgent(state_size, action_size, num_buses, num_routes)\n",
    "    \n",
    "    # Training parameters\n",
    "    episodes = 500\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Tracking metrics\n",
    "    rewards_history = []\n",
    "    avg_rewards_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Choose action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Remember experience\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        # Train the agent\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "        # Update target network every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        # Track rewards\n",
    "        rewards_history.append(total_reward)\n",
    "        avg_reward = np.mean(rewards_history[-100:]) if len(rewards_history) >= 100 else np.mean(rewards_history)\n",
    "        avg_rewards_history.append(avg_reward)\n",
    "        \n",
    "        # Print progress\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode: {episode}, Reward: {total_reward}, Avg Reward: {avg_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards_history, label='Reward')\n",
    "    plt.plot(avg_rewards_history, label='Avg Reward (100 episodes)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return env, agent, rewards_history\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes=10):\n",
    "    \"\"\"Evaluate the trained agent\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Use greedy policy (no exploration)\n",
    "            original_epsilon = agent.epsilon\n",
    "            agent.epsilon = 0\n",
    "            action = agent.act(state)\n",
    "            agent.epsilon = original_epsilon\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Evaluation Episode {episode}: Total Reward = {total_reward}\")\n",
    "    \n",
    "    print(f\"Average Evaluation Reward: {np.mean(total_rewards)}\")\n",
    "    \n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a34240-9027-42fd-832a-125abee674d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training process\n",
    "if __name__ == \"__main__\":\n",
    "    env, agent, rewards_history = train_agent()\n",
    "    eval_rewards = evaluate_agent(env, agent)\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"Final agent performance statistics:\")\n",
    "    print(f\"- Mean reward: {np.mean(rewards_history[-100:])}\")\n",
    "    print(f\"- Std dev reward: {np.std(rewards_history[-100:])}\")\n",
    "    print(f\"- Min reward: {np.min(rewards_history[-100:])}\")\n",
    "    print(f\"- Max reward: {np.max(rewards_history[-100:])}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    agent.model.save(\"ev_scheduling_adp_model.h5\")\n",
    "    print(\"Model saved to ev_scheduling_adp_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3c83b-9388-429f-8e74-f49d0e61232b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d68634-a28a-422b-8ad3-bbdc593e918b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff7ee6-824c-452c-add5-3813c94b693d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99da8f7-3632-4aff-9463-8b7f77d0140c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69980527-819c-48b2-9eb1-2a6c93bf47ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traning ADP: 100%|███████████████████████████████████████████████████████████████| 500/500 [37:51:04<00:00, 272.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal action: charge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "# Environment Parameters\n",
    "MAX_BATTERY = 100  # kWh\n",
    "MAX_DELAY = 20  # minutes\n",
    "NUM_STOPS = 5\n",
    "CHARGE_RATE = 25  # kWh per charging session\n",
    "\n",
    "# Neural Network for Value Function Approximation\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3, 64),  # Input: battery, delay, demand\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ADP Agent\n",
    "class EBusADP:\n",
    "    def __init__(self):\n",
    "        self.value_net = ValueNetwork()\n",
    "        self.optimizer = optim.Adam(self.value_net.parameters(), lr=0.001)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Evaluate possible actions using current value network\"\"\"\n",
    "        actions = ['proceed', 'speed_up', 'skip_stop', 'charge']\n",
    "        values = []\n",
    "        \n",
    "        # Simulate outcomes for each action\n",
    "        for action in actions:\n",
    "            next_state, reward = self.simulate_transition(state, action)\n",
    "            with torch.no_grad():\n",
    "                value = reward + self.gamma * self.value_net(torch.FloatTensor(next_state))\n",
    "            values.append(value.item())\n",
    "            \n",
    "        return actions[np.argmin(values)]  # Minimize cost\n",
    "    \n",
    "    def simulate_transition(self, state, action):\n",
    "        \"\"\"Stochastic state transition model\"\"\"\n",
    "        battery, delay, demand = state\n",
    "        \n",
    "        # Simulate stochastic delay (Gamma distribution)\n",
    "        traffic_delay = np.random.gamma(shape=2, scale=1.5)\n",
    "        \n",
    "        # Simulate stochastic demand (Poisson process)\n",
    "        actual_demand = np.random.poisson(demand)\n",
    "        \n",
    "        # Action effects\n",
    "        if action == 'speed_up':\n",
    "            new_delay = max(0, delay + traffic_delay - 3)\n",
    "            battery_use = 15  # kWh\n",
    "        elif action == 'skip_stop':\n",
    "            new_delay = max(0, delay + traffic_delay - 5)\n",
    "            battery_use = 10\n",
    "            actual_demand = 0  # No boarding\n",
    "        elif action == 'charge':\n",
    "            new_delay = delay + traffic_delay + 10  # Charging time penalty\n",
    "            battery_use = -CHARGE_RATE\n",
    "        else:  # proceed\n",
    "            new_delay = delay + traffic_delay\n",
    "            battery_use = 12\n",
    "            \n",
    "        # Update state\n",
    "        new_battery = max(0, min(MAX_BATTERY, battery - battery_use))\n",
    "        new_delay = min(new_delay, MAX_DELAY)\n",
    "        new_demand = np.random.poisson(8)  # Next stop demand\n",
    "        \n",
    "        # Reward calculation\n",
    "        reward = (\n",
    "            -new_delay * 0.5  # Delay penalty\n",
    "            - max(0, actual_demand - 15) * 0.2  # Overcrowding penalty\n",
    "            - (MAX_BATTERY - new_battery) * 0.1  # Battery penalty\n",
    "        )\n",
    "        \n",
    "        return (new_battery, new_delay, new_demand), reward\n",
    "        \n",
    "    def train(self, episodes=1000):\n",
    "        \"\"\"Training loop with experience replay\"\"\"\n",
    "        for episode in tqdm(range(episodes), desc=\"Traning ADP: \"):\n",
    "            state = (MAX_BATTERY, 0, np.random.poisson(10))  # Initial state\n",
    "            total_cost = 0\n",
    "            \n",
    "            while state[0] > 0:  # Until battery depleted\n",
    "                # Generate experience\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward = self.simulate_transition(state, action)\n",
    "                \n",
    "                # Update neural network\n",
    "                target = reward + self.gamma * self.value_net(torch.FloatTensor(next_state))\n",
    "                prediction = self.value_net(torch.FloatTensor(state))\n",
    "                \n",
    "                loss = nn.MSELoss()(prediction, target.detach())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                state = next_state\n",
    "                total_cost += reward\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = EBusADP()\n",
    "    agent.train(episodes=500)\n",
    "    \n",
    "    # Test policy\n",
    "    test_state = (80, 5, 12)  # 80% battery, 5min delay, 12 expected demand\n",
    "    print(f\"Optimal action: {agent.get_action(test_state)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e902ee4-e1a2-4a93-8ece-837148dbbbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
